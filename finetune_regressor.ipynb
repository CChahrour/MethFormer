{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e9d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PretrainedConfig, Trainer, TrainingArguments\n",
    "\n",
    "from methformer import MethformerRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5e2730",
   "metadata": {},
   "outputs": [],
   "source": [
    "meth_adata = ad.read_h5ad(\"data/methformer_pretraining_dataset.h5ad\")\n",
    "mll_adata = ad.read_h5ad(\"data/mll_n.h5ad\")\n",
    "\n",
    "# Extract patient/sample identifiers\n",
    "meth_ids = meth_adata.obs_names.tolist()\n",
    "ids = [mid.split(\"-\")[2] for mid in meth_ids]\n",
    "\n",
    "# Filter MLL-N IDs by sample matches\n",
    "mll_ids = mll_adata.obs_names.tolist()\n",
    "mll_series = pd.Series(mll_ids)\n",
    "mll_samples = sorted(mll_series[mll_series.str.contains('|'.join(ids))])\n",
    "\n",
    "# Build MethFormer sample â†’ MLL-N replicate mapping\n",
    "meth_to_mll = defaultdict(list)\n",
    "for mll_id in mll_samples:\n",
    "    match_id = mll_id.split(\"-\")[1]\n",
    "    for meth_id in meth_ids:\n",
    "        if match_id in meth_id:\n",
    "            meth_to_mll[meth_id].append(mll_id)\n",
    "\n",
    "meth_to_mll = dict(meth_to_mll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1bb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hold out test patient 23003 and validation patient 22620 ---\n",
    "test_dict = {'METH-patient-23003': meth_to_mll['METH-patient-23003']}\n",
    "val_dict = {'METH-patient-22620': meth_to_mll['METH-patient-22620']}\n",
    "\n",
    "# Remove from training pool\n",
    "for key in ['METH-patient-23003', 'METH-patient-22620']:\n",
    "    meth_to_mll.pop(key)\n",
    "\n",
    "train_dict = meth_to_mll\n",
    "\n",
    "# --- Dataset class ---\n",
    "class MethformerRepAveragedDataset(Dataset):\n",
    "    def __init__(self, meth_adata, mll_adata, match_dict, input_layer=\"methylation\"):\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for meth_id, mll_ids in match_dict.items():\n",
    "            meth_idx = meth_adata.obs_names.get_loc(meth_id)\n",
    "            meth_data = meth_adata.layers[input_layer][meth_idx]  # (515400, 2)\n",
    "            meth_img = np.asarray(meth_data).astype(np.float32).T  # shape: (2, 515400)\n",
    "\n",
    "\n",
    "            mll_rpks = []\n",
    "            for mll_id in mll_ids:\n",
    "                mll_idx = mll_adata.obs_names.get_loc(mll_id)\n",
    "                rpkm = mll_adata[mll_idx].X.astype(np.float32)\n",
    "                mll_rpks.append(rpkm)\n",
    "\n",
    "            mean_rpkm = np.mean(mll_rpks, axis=0)\n",
    "            log_mean_rpkm = np.log1p(mean_rpkm).mean()\n",
    "\n",
    "            self.inputs.append(meth_img)\n",
    "            self.labels.append(log_mean_rpkm)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"pixel_values\": torch.tensor(self.inputs[idx], dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# --- Load datasets ---\n",
    "train_dataset = MethformerRepAveragedDataset(meth_adata, mll_adata, train_dict)\n",
    "val_dataset = MethformerRepAveragedDataset(meth_adata, mll_adata, val_dict)\n",
    "test_dataset = MethformerRepAveragedDataset(meth_adata, mll_adata, test_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867f9470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['regression_head.weight', 'regression_head.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Load config ---\n",
    "config_path = Path(\"output/methformer_2025-05-30_2327/methformer_pretrained/config.json\")\n",
    "with open(config_path) as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "config = PretrainedConfig.from_dict(config_dict)\n",
    "\n",
    "# --- Instantiate model ---\n",
    "model = MethformerRegressor(config)\n",
    "\n",
    "# --- Load safetensors weights ---\n",
    "weights_path = \"output/methformer_2025-05-30_2327/methformer_pretrained/model.safetensors\"\n",
    "state_dict = load_file(weights_path)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc4b3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MethformerRegressor(\n",
       "  (embed): Linear(in_features=2, out_features=128, bias=True)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_head): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (regression_head): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed91178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=1e-4,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"r2\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.flatten()\n",
    "    r2 = r2_score(labels, preds)\n",
    "    mse = mean_squared_error(labels, preds)\n",
    "    return {\"r2\": r2, \"mse\": mse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a8241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc44a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcatherine-chahrour\u001b[0m (\u001b[33mcatherine-chahrour-university-of-oxford\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/project/MethFormer/wandb/run-20250601_103615-whfrht1u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/catherine-chahrour-university-of-oxford/huggingface/runs/whfrht1u' target=\"_blank\">results</a></strong> to <a href='https://wandb.ai/catherine-chahrour-university-of-oxford/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/catherine-chahrour-university-of-oxford/huggingface' target=\"_blank\">https://wandb.ai/catherine-chahrour-university-of-oxford/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/catherine-chahrour-university-of-oxford/huggingface/runs/whfrht1u' target=\"_blank\">https://wandb.ai/catherine-chahrour-university-of-oxford/huggingface/runs/whfrht1u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: input_values,attention_mask,kwargs,label,label_ids.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bertnado/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bertnado/lib/python3.11/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bertnado/lib/python3.11/site-packages/transformers/trainer.py:3730\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3727\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.optimizer, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.optimizer.train):\n\u001b[32m   3728\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.train()\n\u001b[32m-> \u001b[39m\u001b[32m3730\u001b[39m inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   3732\u001b[39m     loss_mb = smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bertnado/lib/python3.11/site-packages/transformers/trainer.py:3679\u001b[39m, in \u001b[36mTrainer._prepare_inputs\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m   3677\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m._prepare_input(inputs)\n\u001b[32m   3678\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3679\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3680\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe batch received was empty, your model won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be able to train on it. Double-check that your \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3681\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtraining dataset contains keys expected by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(\u001b[38;5;28mself\u001b[39m._signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3682\u001b[39m     )\n\u001b[32m   3683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3684\u001b[39m     inputs[\u001b[33m\"\u001b[39m\u001b[33mmems\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._past\n",
      "\u001b[31mValueError\u001b[39m: The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: input_values,attention_mask,kwargs,label,label_ids."
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6a097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertnado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
